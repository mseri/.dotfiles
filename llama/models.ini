; models.ini
version = 1

; Global settings
[*]
n-gpu-layers = 0
models-max = 1
threads = 4
jinja = true
cache-reuse = 256
ctx-size = 8192

[unsloth/granite-4.0-h-micro-GGUF:UD-Q4_K_XL]
alias = granite4
top-k = 20
top-p = 0.95
min-p = 0.0
temp = 0
ctx-size = 16384

[unsloth/granite-4.0-h-tiny-GGUF:UD-Q4_K_XL]
alias = granite4-moe
top-k = 20
top-p = 0.95
min-p = 0.0
temp = 0
ctx-size = 16384

[stduhpf/google-gemma-3-4b-it-qat-q4_0-gguf-small]
alias = gemma3
top-k = 64
top-p = 0.95
min-p = 0.0
repeat-penalty = 1.0
temp = 1.0
ctx-size = 16384

[LiquidAI/LFM2-2.6B-GGUF:Q8_0]
alias = lfm2
temp = 0.3
min-p = 0.15
repeat-penalty = 1.05
ctx-size = 16384

[LiquidAI/LFM2-2.6B-Exp-GGUF:Q8_0]
alias = lfm2-exp
temp = 0.3
min-p = 0.15
repeat-penalty = 1.05
ctx-size = 16384

[unsloth/LFM2-8B-A1B-GGUF:Q4_K_XL]
alias = lfm2-moe
temp = 0.3
min-p = 0.15
repeat-penalty = 1.05
ctx-size = 16384

[bartowski/LiquidAI_LFM2-VL-1.6B-GGUF:Q6_K]
alias = lfm2-vl
temp = 0.1
min-p = 0.15
top-p = 1.0
top-k = 50
repeat-penalty = 1.05
ctx-size = 16384

[mistralai/Ministral-3-3B-Instruct-2512-GGUF:Q4_K_M]
alias = ministral3-3i
temp = 0.15
ctx-size = 16384

[mistralai/Ministral-3-3B-Reasoning-2512-GGUF:Q4_K_M]
alias = ministral3-3r
top-p = 0.95
temp = 0.7
ctx-size = 16384

[mistralai/Ministral-3-8B-Instruct-2512-GGUF:Q4_K_M]
alias = ministral3-8i
temp = 0.15
ctx-size = 16384

[mistralai/Ministral-3-8B-Reasoning-2512-GGUF:Q4_K_M]
alias = ministral3-8r
top-p = 0.95
temp = 0.7
ctx-size = 16384

[bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF:Q4_K_M]
alias = nemotron
temp = 0.6
top-p = 0.95
ctx-size = 16384

[unsloth/Qwen3-4B-Instruct-2507-GGUF:Q4_K_XL]
alias = qwen3i
top-k = 20
top-p = 0.8
min-p = 0.0
temp = 0.7

[unsloth/Qwen3-4B-Thinking-2507-GGUF:Q4_K_XL]
alias = qwen3r
top-k = 20
top-p = 0.95
min-p = 0.0
temp = 0.6

